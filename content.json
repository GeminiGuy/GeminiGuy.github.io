{"meta":{"title":"隔壁老王的Blog","subtitle":null,"description":"嘘，要发车了","author":"Gemini","url":"https://geminiguy.github.io"},"pages":[{"title":"categories","date":"2017-01-14T09:04:41.000Z","updated":"2017-01-14T09:12:51.000Z","comments":false,"path":"categories/index.html","permalink":"https://geminiguy.github.io/categories/index.html","excerpt":"","text":""},{"title":"tags","date":"2017-01-14T05:54:18.000Z","updated":"2017-01-14T06:02:54.000Z","comments":false,"path":"tags/index.html","permalink":"https://geminiguy.github.io/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"Kafka Controller","slug":"Kafka-Controller","date":"2017-01-12T16:00:00.000Z","updated":"2017-01-14T09:24:53.000Z","comments":true,"path":"2017/01/13/Kafka-Controller/","link":"","permalink":"https://geminiguy.github.io/2017/01/13/Kafka-Controller/","excerpt":"前言目前网上针对Kafka Controller的分析文章不算少，但是大多比较散乱，不够系统，因此趁着这次部门分享Kafka的机会，将一些知识点进行了梳理总结，学习过程中参考了一些原理性的分析文章和0.10版本的源码。本文埋了一些细节坑，待有时间再回来补全吧。对于Kafka的研究水平有限，如果文章中出现错误，还望在评论区交流指正，谢谢。下面我们首先从基础概念开始讲起。","text":"前言目前网上针对Kafka Controller的分析文章不算少，但是大多比较散乱，不够系统，因此趁着这次部门分享Kafka的机会，将一些知识点进行了梳理总结，学习过程中参考了一些原理性的分析文章和0.10版本的源码。本文埋了一些细节坑，待有时间再回来补全吧。对于Kafka的研究水平有限，如果文章中出现错误，还望在评论区交流指正，谢谢。下面我们首先从基础概念开始讲起。 基础概念 什么是Controller： 从Kafka集群中选取一个broker作为controller，负责管理topic分区和副本的状态的变化，以及执行重分配分区之类的管理任务。 Replication总共经历了三个版本的迭代，其优点如下： 解决了Zk的split-brain问题，分区状态的一致性得到保障 更少的ZK监听器，减轻了Zk负载以及herd effect问题 Leader的变化针对Zk的读写可以批量操作，减少failover过程中的延迟 Controller将状态的改变直接通过RPC的方式通知相应broker，相比ZK队列方式更高效 名词解释 AR 当前已分配的副本列表 RAR 重分配过的副本列表 ORA 重分配之前的副本列表 分区Leader 给定分区负责客户端读写的结点 ISR “in-sync” replicas，能够与Leader保持同步的副本集合（Leader也在ISR中） 和controller相关的zookeeper路径说明 /controller：结点存放当前Controller信息 /brokers/ids/[brokerId]：存放集群broker信息，包括ip、端口、jmx信息 /brokers/topics：子目录为topic列表 /brokers/topics/[topic]：存放对应topic的各个分区AR /brokers/topics/[topic]/partitions/[partitionId]/state：结点存放当前topic各个分区的Leader、ISR、controller_epoch、leader_epoch等信息。 /admin临时结点，只有相关操作时才会存在 /admin/delete_topics： 要删除的一些topic /admin/reassign_partitions：指导重新分配AR的路径，通过命令修改AR时会写入到这个路径下 /admin/preferred_replica_election：分区需要重新选举第一个replica作为leader，即preferred replica。起到自动平衡Leader分配的作用，使用自带工具或者auto.leader.rebalance.enable=true均可 LeaderAndIsrRequest,UpdateMetadataRequest,StopReplicaRequestLeaderAndIsrRequest请求及响应123456789101112131415161718192021222324/*** controllerId：controller所在的brokerId* controllerEpoch：controller选举版本号* partitionStates：Map[(topic, partitionId), PartitionState]* liveLeaders：传入参数分区leader所在的broker集合*/public LeaderAndIsrRequest(int controllerId, int controllerEpoch, Map&lt;TopicPartition, PartitionState&gt; partitionStates,Set&lt;Node&gt; liveLeaders) &#123;...&#125;/*** controllerEpoch：controller选举版本号* leader：分区leader所在的broker Id* leaderEpoch：分区leader选举版本号* isr：当前ISR列表* zkVersion：分区在zookeeper上的状态信息，用作竞态更新* replicas：分区副本列表*/public PartitionState(int controllerEpoch, int leader, int leaderEpoch, List&lt;Integer&gt; isr, int zkVersion, Set&lt;Integer&gt; replicas) &#123; this.controllerEpoch = controllerEpoch; this.leader = leader; this.leaderEpoch = leaderEpoch; this.isr = isr; this.zkVersion = zkVersion; this.replicas = replicas;&#125; 请求响应流程：makeLeaders、makeFollowers具体逻辑留坑待填 UpdateMetadataRequest请求及响应12345678/*** 可以看到此构造函数和LeaderAndIsrRequest请求类构造函数参数基本一致* liveBrokers：存活的broker集合*/public UpdateMetadataRequest(int version, int controllerId, int controllerEpoch, Map&lt;TopicPartition, PartitionState&gt; partitionStates, Set&lt;Broker&gt; liveBrokers)请求响应：每个broker都维护了相同的缓存，更新MetadataCache实例中的chace字段和aliveBroker字段分别为topic对应的分区状态信息以及当前可用的broker列表 StopReplicaRequest请求及响应1234public StopReplicaRequest(int controllerId, int controllerEpoch, boolean deletePartitions, Set&lt;TopicPartition&gt; partitions)请求响应：1. ReplicaManager会移除replica所在的partition的Fetcher，即不再向该partition的leader拉取新的数据2. 再根据deletePartitions决定是否删除该replica对应的topic partition日志 Partition状态机 PartitionState四种分区状态如下： NonExistentPartition：该分区要么没有被创建过或曾经被创建过但后面删除了 NewPartition：分区创建之后已经分配了副本，但是还没有选举出Leader和ISR OnlinePartition：分区Leader一旦被选举出来，就处在该状态 OfflinePartition：如果leader选举出来后，leader broker宕机了，那么该分区就处于OfflinePartition状态 分区状态转换图针对OnlinePartition,OfflinePartition –&gt; OnlinePartition而言，有四种Leader选举器策略如下: OfflinePartitionLeaderSelector If at least one broker from the isr is alive, it picks a broker from the live isr as the new leader and the live isr as the new isr. Else, if unclean leader election for the topic is disabled, it throws a NoReplicaOnlineException. Else, it picks some alive broker from the assigned replica list as the new leader and the new isr. If no broker in the assigned replica list is alive, it throws a NoReplicaOnlineException Replicas to receive LeaderAndIsr request = live assigned replicas，Once the leader is successfully registered in zookeeper, it updates the allLeaders cache ReassignedPartitionLeaderSelector New leader = a live in-sync reassigned replicaNew isr = current isrReplicas to receive LeaderAndIsr request = reassigned replicas PreferredReplicaPartitionLeaderSelector New leader = preferred (first assigned) replica (if in isr and alive)New isr = current isrReplicas to receive LeaderAndIsr request = assigned replicas ControlledShutdownLeaderSelector New leader = replica in isr that’s not being shutdownNew isr = current isr - shutdown replicaReplicas to receive LeaderAndIsr request = live assigned replicas Replica状态机 ReplicaState副本状态如下： NewReplica：当创建了topic或者重分配分区时Controller会创建新的副本，就处在这个状态，此状态中的副本只能接收“成为follower”的状态变更请求，可由NonExistentReplica转换而来 OnlineReplica：一旦启动了一个副本以及该分区AR副本集合中的一部分，那么就将设置该副本状态为OnlineReplica。在此状态中的副本可以接收”成为leader”或”成为follower”的状态变更请求。可由NewRelica、OnlineReplica或OfflineReplica状态转换而来 OfflineReplica：如果一个副本挂掉(保存该副本的broker宕机)将被置于OfflineReplica状态，可由NewReplica或OnlineReplica状态转换而来 ReplicaDeletionStarted：开启副本删除操作时会将副本状态置于ReplicaDeletionStarted状态，可由OfflineReplica状态转换而来 ReplicaDeletionSuccessful：如果副本删除请求成功，返回的响应没有错误的话，该副本会被置于ReplicaDeletionSuccessful状态，可由ReplicaDeletionStarted状态转换而来 ReplicaDeletionIneligible：如果副本删除失败，将被置于ReplicaDeletionIneligible状态，可由ReplicaDeletionStarted状态转换而来 NonExistentReplica：如果副本被成功删除将被置于NonExistentReplica状态，可由ReplicaDeletionSuccessful状态转换而来 副本状态转换图 举例分析创建topic 命令行逻辑 12345bin/kafka-topics.sh --zookeeper XXX:12181 --create --topic fanstop_messagecore --partitions 2 --replication-factor 3a.确定分区副本的分配方案0 --&gt; [2,1,3]1 --&gt; [3,2,4]b.创建Zk结点，将分配方案写到 /brokers/topics/fanstop_messagecore 结点下 Controller后台逻辑 Controller启动时,分区状态机PartitionStateMachine.registerListeners()会注册一些监听器，当用命令行新增加了/brokers/topics/fanstop_messagecore结点后，就会触发监听器TopicChangeListener的handleChildChange方法，逻辑如下： 获取/brokers/topics路径下集合，与当前controller中保存的topic集合比较，找出新增topic集合，比如fanstop_messagecore（新增or删除） 更新controller的topic缓存列表 从/brokers/topics/fanstop_messagecore结点取出这个topic所有分区的分配方案，更新controller对应的缓存信息 调用KafkaController.onNewTopicCreation方法 onNewTopicCreation 123456def onNewTopicCreation(topics: Set[String], newPartitions: Set[TopicAndPartition]) &#123; info(\"New topic creation callback for %s\".format(newPartitions.mkString(\",\"))) // subscribe to partition changes topics.foreach(topic =&gt; partitionStateMachine.registerPartitionChangeListener(topic)) onNewPartitionCreation(newPartitions)&#125; 为每一个topic注册/brokers/topics/[topic]/下的分区变更监听器PartitionModificationsListener，本次仅注册但不会触发 调用onNewPartitionCreation方法创建分区 onNewPartitionCreation 1234567def onNewPartitionCreation(newPartitions: Set[TopicAndPartition]) &#123; info(\"New partition creation callback for %s\".format(newPartitions.mkString(\",\"))) partitionStateMachine.handleStateChanges(newPartitions, NewPartition) replicaStateMachine.handleStateChanges(controllerContext.replicasForPartition(newPartitions), NewReplica) partitionStateMachine.handleStateChanges(newPartitions, OnlinePartition, offlinePartitionSelector) replicaStateMachine.handleStateChanges(controllerContext.replicasForPartition(newPartitions), OnlineReplica)&#125; 创建新增分区对象，分区状态机将新创建分区转变为NewPartition状态 从Controller缓存中取出新增分区的副本分配方案，针对每一个replica进行NonExistentReplica–&gt; NewReplica状态转换 分区状态 NewPartition–&gt;OnlinePartition，调用initializeLeaderAndIsrForPartition(topicAndPartition)为新分区初始化leader和isr路径。主要策略就是让AR中第一个replica作为leader，所有可用的replica作为ISR，将leader、ISR、leaderepoch、controllerepoch、zkversion等信息写入Zk state节点，ControllerContext更新相应topic分区的leader和ISR缓存。之后就是发送LeaderAndIsrRequest给每个replica和UpdateMetadataRequest请求给每个可用的broker 设置副本状态机状态NewReplica—&gt;OnlineReplica，将当前的replica加入到controllerContext AR缓存中。完事…… Broker Failure下面来分析一下某台broker宕机的情况。 broker宕机后，zk会话超时，/brokers/ids对应结点被删除，触发BrokerChangeListener.handleChildChange，接着又调用controller.onBrokerFailure方法， 对Leader副本在deadBroker上的分区（同时要排除topic在删除的状态）触发OfflinePartition状态转换 这一步就是为处于OfflinePartition状态的分区触发OnlinePartition状态转换，ok，还记得上面提到的OfflinePartitionLeaderSelector选举器吧，这回就用它来进行Leader选举、ISR重分配，以及选出接收LeaderAndIsr请求的replica。更新一下Zk state结点的leader和ISR相关信息，更新controllerContext的leader缓存，给相应broker发送LeaderAndIsrRequest请求，顺带发送UpdateMetadataRequest请求，这些动作都是在electLeaderForPartition函数中完成 找出deadBroker上的所有副本，并且要过滤掉一些topics正在执行删除操作的replica，副本状态机执行OfflineRelica状态转换，过程不再赘述 check出正在执行topic删除期间的replica，将这些replica置为ReplicaDeletionIneligible，这些replica在broker down的时候是不能被删除的。 分区副本重新分配策略Kafka提供了分区重新分配的工具，在生产环境下，随着负载的增大，可能需要给Kafka集群扩容，但是对于已存在的topic，并不会自动将分区的副本迁移到新的Broker上，就可以用到这个工具。除此之外，我们还可以用这个工具来调整分区的AR数量，下面就来说一下Partition方案重新分配的情况，也涉及Controller非常重要的一个方法：onPartitionReassignment。当用命令行发起分区重分配操作时，它会在/admin/reassign_partitions路径下创建节点，进而触发PartitionsReassignedListener监听器，执行handleDataChange函数，过滤掉正在执行重分配的副本的分区，接着判断它们的topic是否正在进行删除，构造一个ReassignedPartitionsContext传递给controller.initiateReassignReplicasForTopicPartition为给定topic分区重新分配副本，这个函数的逻辑看了下源码，基本流程如下： 取出controllerContext针对这个分区的AR缓存，当前AR和RAR集合完全一样的的话，抛异常，不用重分配了 否则，判断一下RAR中是否存在不可用的副本，如果存在replica not alive，抛异常，此次重分配操作无法进行 如果RAR中均可用，注册Zk state结点的listen，监听ISR变化，mark一下该topic不能被删除。 最后一步调用最重要的函数onPartitionReassignment开始重新分配分区的副本 onPartitionReassignment这里我们还是以源码中的例子来说明整个流程:OAR = {1,2,3}RAR = (4,5,6) AR(Zk) leader/isr(Zk) transition {1,2,3} 1/{1,2,3} (initial state) {1,2,3,4,5,6} 1/{1,2,3} (step 2) {1,2,3,4,5,6} 1/{1,2,3,4,5,6} (step 4) {1,2,3,4,5,6} 4/{1,2,3,4,5,6} (step 7) {1,2,3,4,5,6} 4/{4,5,6} (step 8) {4,5,6} 4/{4,5,6} (step 10) 总结本文只举了三个经典的例子，后面再慢慢扩充用到Controller的实例分析吧。对于之前连Scala语法都没接触过的老王来说，整个调研学习过程真可谓折磨蛋疼额，技术无止境，共勉吧。However，还是希望本文能对读者有所帮助。 参考文献 官方wiki：Kafka Controller Internals 推荐Kafka相关博客 Kafka Controller源码分析","categories":[{"name":"技术","slug":"技术","permalink":"https://geminiguy.github.io/categories/技术/"}],"tags":[{"name":"Kafka Controller","slug":"Kafka-Controller","permalink":"https://geminiguy.github.io/tags/Kafka-Controller/"},{"name":"源码分析","slug":"源码分析","permalink":"https://geminiguy.github.io/tags/源码分析/"},{"name":"Kafka","slug":"Kafka","permalink":"https://geminiguy.github.io/tags/Kafka/"}]},{"title":"Hello World","slug":"index","date":"2016-12-31T16:00:00.000Z","updated":"2017-01-14T09:27:00.000Z","comments":true,"path":"2017/01/01/index/","link":"","permalink":"https://geminiguy.github.io/2017/01/01/index/","excerpt":"","text":"Hello world，隔壁老王新的一年博客开张个人比较懒，平时知识点也只是随手记录在印象笔记希望这一年能多写一些技术文章吧Whatever，新年快乐Ps：Hexo的环境配置真是好多坑啊，还是比较喜欢NexT这个主题","categories":[{"name":"闲聊","slug":"闲聊","permalink":"https://geminiguy.github.io/categories/闲聊/"}],"tags":[{"name":"闲聊","slug":"闲聊","permalink":"https://geminiguy.github.io/tags/闲聊/"}]}]}